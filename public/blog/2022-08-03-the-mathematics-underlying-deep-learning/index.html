<!DOCTYPE html>
<html lang="en" dir="ltr"><head>
  
                           
     


<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="generator" content="Hugo 0.101.0" />
<title>The Mathematics Underlying Deep Learning | Bas Machielsen</title>


<meta property="twitter:site" content="@basss92">
<meta property="twitter:creator" content="@basss92">







  
    
  
<meta name="description" content="Working out the underlying mathematical details of standard neural networks.">


<meta property="og:site_name" content="Bas Machielsen">
<meta property="og:title" content="The Mathematics Underlying Deep Learning | Bas Machielsen">
<meta property="og:description" content="Working out the underlying mathematical details of standard neural networks." />
<meta property="og:type" content="page" />
<meta property="og:url" content="/blog/2022-08-03-the-mathematics-underlying-deep-learning/" />
<meta property="og:locale" content="en">




    
        <meta property="og:image" content="/blog/2022-08-03-the-mathematics-underlying-deep-learning/featured.png" >
        <meta property="twitter:card" content="summary_large_image">
        <meta name="twitter:image" content="/blog/2022-08-03-the-mathematics-underlying-deep-learning/featured.png" >
    
    
  <meta itemprop="name" content="The Mathematics Underlying Deep Learning">
<meta itemprop="description" content="Introduction In this post, I want to provide a quick primer about the mathematics underlying the backpropagation algorithm in neural networks and deep learning algorithms. I use the notation provided in this book, chapter 2, and prove/explain in-depth where some of the equations come from. I hope this provides an intuitive and clear explanation about the logic underlying backpropagation, and it should make it easy to implement this using code."><meta itemprop="datePublished" content="2021-05-29T00:00:00+00:00" />
<meta itemprop="dateModified" content="2021-05-29T00:00:00+00:00" />
<meta itemprop="wordCount" content="797"><meta itemprop="image" content="/blog/2022-08-03-the-mathematics-underlying-deep-learning/featured.png">
<meta itemprop="keywords" content="" />
  
  
  <!--[if IE]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  <link rel="shortcut icon" href="favicon_new.ico" type="image/x-icon">
  <link rel="icon" href="favicon_new.ico" type="image/x-icon">
  
  
  <link rel="stylesheet" href="/style.main.min.4eae612a1c2fbef32d12b5ae071d8dfd291d182f14e2efa82403d581b3b4419e.css" integrity="sha256-Tq5hKhwvvvMtErWuBx2N/SkdGC8U4u&#43;oJAPVgbO0QZ4=" media="screen">
  
  
  <script src="/panelset.min.dca42702d7daf6fd31dc352efd2bcf0e4ac8c05ccaa58d9293f6177462de5d5f.js" type="text/javascript"></script>
  
  
  <script src="/main.min.d7e4e8181380eb79c579f1573ca10b3aeb727c619c43ff775fec0b0d1624b24d.js" type="text/javascript"></script>
</head>
<body>
      <div class="grid-container single">
<header class="site-header pt4 pb2 mb4 bb b--transparent ph5 headroom z-max" role="banner">
  <nav class="site-nav db dt-l w-100" role="navigation">
    <a class="site-brand db dtc-l v-mid link no-underline w-100 w-33-l tc tl-l" href="/" title="Home">
      <img src="/website_logo.png" class="dib db-l h2 w-auto" alt="Bas Machielsen">
    </a>
    <div class="site-links db dtc-l v-mid w-100 w-47-l tc tr-l mt3 mt0-l ttu tracked">
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/" title="Home">Home</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/about/" title="About">About</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 active" href="/blog/" title="Blog">Blog</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/project/" title="Teaching Overview &amp; Repository">Teaching</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/resume/" title="Resume">Resume</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/articles/" title="Articles">Articles</a>
      
        
        
        
      <a class="link f6 f5-l dib pv1 ph2 " href="/collection/" title="Software">Software</a>
      
      
    </div>
  </nav>
</header>

<main class="page-main pa4" role="main">
  <section class="page-content mw7 center">
    <article class="post-content pa0 ph4-l">
      <header class="post-header">
        <h1 class="f1 lh-solid measure-narrow mb3 fw4">The Mathematics Underlying Deep Learning</h1>
        
        <p class="f6 measure lh-copy mv1">By Bas Machielsen</p>
        <p class="f7 db mv0 ttu">May 29, 2021</p>

      

      </header>
      <section class="post-body pt5 pb4">
        



<h2 id="introduction">Introduction
  <a href="#introduction"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>In this post, I want to provide a quick primer about the mathematics underlying the backpropagation algorithm in neural networks and deep learning algorithms. I use the notation provided in 
<a href="http://neuralnetworksanddeeplearning.com/" target="_blank" rel="noopener">this book</a>, chapter 2, and prove/explain in-depth where some of the equations come from. I hope this provides an intuitive and clear explanation about the logic underlying backpropagation, and it should make it easy to implement this using code.</p>




<h2 id="delta">Delta
  <a href="#delta"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p><code>\(\delta^l\)</code> is defined as <code>\(\frac{\partial C}{\partial z^l}\)</code>, or in component form: <code>\(\delta^l_j = \frac{\partial C}{\partial z^l_j}\)</code>. The network is define such that the matrix <code>\(W^L\)</code> represents the weights from layer <code>\(a^{l-1}\)</code> to layer <code>\(a^l\)</code>. We start indexation at 1 (so not at 0!). Hence, layer <code>\(a^0\)</code> and weights <code>\(W^1\)</code> do not exist, and layer <code>\(a^1\)</code> is the input layer. There are a total of <code>\(L\)</code> layers in the network, each of which can have a variable number of nodes. First, we find an expression for <code>\(\delta^L\)</code>, meaning delta in the last layer.</p>
<p>By the chain rule, and recognizing that the only way <code>\(z^L_j\)</code> influences the cost function is through <code>\(a^L_j\)</code>, we know that:</p>
<p>$$
\delta^L_j = \frac{\partial C}{\partial a^L_j} \cdot \frac{\partial a^L_j}{\partial z^L_j}
$$</p>
<p>The first part of this derivative is determined by the cost function, and the second part <code>\(\frac{\partial a^L_j}{\partial z^L_j}\)</code> evaluates to <code>\(\sigma'(z^L_j)\)</code>. Hence, in matrix form, the expression for <code>\(\delta^L\)</code> is:</p>
<p>$$
\begin{pmatrix}
\vdots \newline
\delta^L_j \newline
\vdots
\end{pmatrix} =
\begin{pmatrix}
\frac{\partial C}{\partial a^L_1} \newline
\vdots \newline
\frac{\partial C}{\partial a^L_j} \newline
\vdots
\end{pmatrix} \circ \begin{pmatrix} \sigma&rsquo;(z^L_1) \newline
\vdots \newline
\sigma&rsquo;(z^L_j) \newline
\vdots
\end{pmatrix}
$$</p>
<p>or alternatively, simply:</p>
<p>$$
\delta^L = \frac{\partial C}{\partial a^L} \cdot \sigma&rsquo;(z^L)
$$</p>




<h2 id="from-layer-l-to-layer-l-1">From layer L to layer L-1
  <a href="#from-layer-l-to-layer-l-1"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>As soon as we have a <code>\(\delta\)</code> for layer <code>\(L\)</code>, or layer <code>\(l\)</code> more generally, we want to find a delta for the previous layer. In the next subsection, it will become clear why. To fix ideas, suppose there are <code>\(K\)</code> elements in layer <code>\(l+1\)</code>, indexed by <code>\(k\)</code> and there are <code>\(J\)</code> elements in layer <code>\(l\)</code>, indexed by <code>\(j\)</code>. Then, we start off with the definition of <code>\(\delta^l\)</code> in component form, which we recognize to be composed of all weights that proceed from that specific node, to one of the nodes <code>\(1\)</code> to <code>\(K\)</code> in layer <code>\(l+1\)</code>:</p>
<p>$$
\delta^l_j = \frac{\partial C}{\partial z^l_j} = \sum_k \frac{\partial C}{\partial z^{l+1)}_k} \cdot \frac{\partial z^{l+1}_k}{z^l_j}
$$</p>
<p>The first term is by definition equal to <code>\(\delta^{l+1}_k\)</code>. The second term can be obtained by recognizing that:</p>
<p>$$
z^{l+1}_k = \sum_j w^{l+1}_{kj} a^l_j = \sum_j w^{l+1}_{kj} \sigma(z^l_j)
$$</p>
<p>Differentiating this expression with respect to <code>\(z^l_j\)</code> gives (because there is only 1 specific <code>\(z^l_j\)</code>):</p>
<p>$$
\frac{\partial z^{l+1}_k}{\partial z^l_j}=w^{l+1}_{kj}\sigma&rsquo;(z^l_j)
$$</p>
<p>Substituting this expression back in the expression for <code>\(\delta^l_j\)</code> gives:</p>
<p>$$
\delta^l_j = \sum_k \delta^{l+1}_k \cdot w_{kj}^{l+1} \cdot \sigma&rsquo;(z^l_j)
$$</p>
<p>In matrix form, this expression becomes:</p>
<p>$$
\delta^l = W_{kj}^T \delta^{l+1} \circ \sigma&rsquo;(z^l)
$$</p>
<p>or, more explicitly,</p>
<p>$$
\begin{bmatrix}
\delta^l_1 \newline
\vdots \newline
\delta^l_J
\end{bmatrix} =
\begin{bmatrix}
w_{11} &amp; w_{21} &amp; \dots &amp; w_{k1} \newline
w_{12} &amp; \dots &amp; \dots &amp; w_{k2} \newline
\vdots &amp; \ddots &amp; \ddots &amp; \vdots \newline
w_{1j} &amp; \dots &amp; \dots &amp; w_{kj}
\end{bmatrix}
\begin{bmatrix}
\delta^{l+1}_1 \newline
\vdots \newline
\delta^{l+1_K}
\end{bmatrix} \circ
\begin{pmatrix}
\sigma&rsquo;(z^l_1) \newline
\vdots \newline
\sigma&rsquo;(z^l_j)
\end{pmatrix}
$$</p>
<p>with the dimensions <code>\(j\)</code> x <code>\(1\)</code> (layer <code>\(l\)</code>) = <code>\(j\)</code> x <code>\(k\)</code> (transpose of the weight matrix) times <code>\(k\)</code> x <code>\(1\)</code> (layer <code>\(l+1\)</code>) times <code>\(j\)</code> x 1 (layer <code>\(l\)</code>.</p>




<h2 id="relating-deltas-to-derivatives">Relating Delta&rsquo;s to Derivatives
  <a href="#relating-deltas-to-derivatives"><svg class="anchor-symbol" aria-hidden="true" height="26" width="26" viewBox="0 0 22 22" xmlns="http://www.w3.org/2000/svg">
      <path d="M0 0h24v24H0z" fill="currentColor"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg></a>
</h2>
<p>Now that we have an expression for all <code>\(\delta^l\)</code>&rsquo;s, we have to relate the weight derivatives <code>\(\frac{\partial C}{\partial w_{jk}}\)</code> to the <code>\(\delta\)</code>&rsquo;s. To see this, we realize that a particular weight to layer <code>\(l\)</code> only influences one particular node <code>\(z_j^l\)</code> in layer <code>\(l\)</code>. Hence, we can write the partial derivative that we are looking for as:</p>
<p>$$
\frac{\partial C}{\partial w^l_{jk}} = \frac{\partial C}{\partial z^l_j} \cdot \frac{\partial z^l_j}{\partial w^l_{jk}}
$$
Then, by definition, <code>\(\frac{\partial C}{\partial z^l_j} = \delta^l_j\)</code>. For the second partial derivative, we realize that, for a layer <code>\(l\)</code> with <code>\(J\)</code> nodes and a layer <code>\(l-1\)</code> with <code>\(K\)</code> nodes:</p>
<p>$$
\begin{pmatrix}
z^l_1 \newline
\vdots \newline
z^l_J \end{pmatrix} =
\begin{pmatrix}
w_{11} &amp; \dots &amp; w_{1K} \newline
\vdots &amp; \ddots &amp; \vdots \newline
w_{J1} &amp; \dots &amp; w_{JK}
\end{pmatrix}
\begin{pmatrix}
a^{l-1}_1 \newline
\vdots \newline
a^{l-1}_K
\end{pmatrix}
$$
And thus, the derivative of <code>\(z^l_j\)</code> with respect to <code>\(w_{jk}\)</code> is just <code>\(a^{l-1}_k\)</code>. Substituting that in the expression for <code>\(\frac{\partial C}{\partial w_{jk}}\)</code> gives:</p>
<p>$$
\frac{\partial C}{\partial w_{jk}} = \delta_j \cdot a^{l-1}_k
$$</p>
<p>Finally, in matrix form, this amounts to:</p>
<p>$$
\begin{pmatrix} \frac{\partial C}{\partial w_{jk}} \end{pmatrix} =
\begin{pmatrix}
\delta^l_1 \newline
\vdots \newline
\delta^l_j
\end{pmatrix}
\cdot
\begin{pmatrix}
a^{l-1}_1 \newline
\vdots \newline
a^{l-1}_K
\end{pmatrix}^T
$$</p>
<p>Which gives back a <code>\(J\)</code> by <code>\(K\)</code> matrix, which after multiplying exactly corresponds to the component form sketched out above. This is also what you would use in an implementation of backpropagation in code.</p>

        
        <details closed class="f6 fw7 input-reset">
  <dl class="f6 lh-copy">
    <dt class="fw7">Posted on:</dt>
    <dd class="fw5 ml0">May 29, 2021</dd>
  </dl>
  <dl class="f6 lh-copy">
    <dt class="fw7">Length:</dt>
    <dd class="fw5 ml0">4 minute read, 797 words</dd>
  </dl>
  
  
  
  <dl class="f6 lh-copy">
    <dt class="fw7">See Also:</dt>
    
  </dl>
</details>

      </section>
      <footer class="post-footer">
        <div class="post-pagination dt w-100 mt4 mb2">
  
  
    <a class="prev dtc pr2 tl v-top fw6"
    href="/blog/2022-08-03-a-from-scratch-implementation-of-neural-networks-in-python/">&larr; A From-Scratch Implementation of Neural Networks in Python</a>
  
  
  
    <a class="next dtc pl2 tr v-top fw6"
    href="/blog/2022-08-03-a-few-game-theory-exercises-and-answers/">A few game theory exercises and answers &rarr;</a>
  
</div>

      </footer>
    </article>
    
      
<div class="post-comments pa0 pa4-l mt4">
  
  <script src="https://utteranc.es/client.js"
          repo="basm92/new_website"
          issue-term="pathname"
          theme="boxy-light"
          label="comments :crystal_ball:"
          crossorigin="anonymous"
          async
          type="text/javascript">
  </script>
  
</div>

    
  </section>
</main>
<footer class="site-footer pv4 bt b--transparent ph5" role="contentinfo">
  <nav class="db dt-l w-100">
    <p class="site-copyright f7 db dtc-l v-mid w-100 w-33-l tc tl-l pv2 pv0-l mv0 lh-copy">
      &copy; 2024 Bas Machielsen
      <span class="middot-divider"></span>
      Made with <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/hugo-apero/" rel="dct:source">Hugo Ap√©ro</a></span>.
      <br />
      
Based on <span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"><a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/formspree/blogophonic-hugo" rel="dct:source">Blogophonic</a></span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://formspree.io" property="cc:attributionName" rel="cc:attributionURL">Formspree</a>.
    </p>
    
    <div class="site-social-links db dtc-l v-mid w-100 w-33-l tc pv2 pv0-l mv0">
      <div class="social-icon-links" aria-hidden="true">
  
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://github.com/basm92" title="github" target="_blank" rel="noopener">
      <i class="fab fa-github fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://scholar.google.com/citations?user=bS8uo44AAAAJ" title="google" target="_blank" rel="noopener">
      <i class="fab fa-google fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://twitter.com/basss92" title="twitter" target="_blank" rel="noopener">
      <i class="fab fa-twitter fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="mailto:a.h.machielsen@uu.nl" title="envelope" >
      <i class="fas fa-envelope fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="mailto:basmachielsen@live.nl" title="envelope" >
      <i class="far fa-envelope fa-lg fa-fw"></i>
    </a>
  
    
    
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="https://orcid.org/0000-0002-9692-0615" title="orcid" target="_blank" rel="noopener">
      <i class="ai ai-orcid fa-lg fa-fw"></i>
    </a>
  
    
    
    
      
    
    
    
    
    
      
    
    <a class="link dib h1 w1 ml0 mr2 f6 o-90 glow" href="/blog/index.xml" title="rss" >
      <i class="fas fa-rss fa-lg fa-fw"></i>
    </a>
  
</div>

    </div>
    
    <div class="site-links f6 db dtc-l v-mid w-100 w-67-l tc tr-l pv2 pv0-l mv0">
      
      <a class="dib pv1 ph2 link" href="/license/" title="License">License</a>
      
      <a class="dib pv1 ph2 link" href="/contact/" title="Contact form">Contact</a>
      
      <a class="dib pv1 ph2 link" href="/contributors/" title="Contributors">Contributors</a>
      
    </div>
  </nav>
  
    <script>

    var i, text, code, codes = document.getElementsByTagName('code');
    for (let i = 0; i < codes.length;) {
      code = codes[i];
      if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
        text = code.textContent;
        if (/^\$[^$]/.test(text) && /[^$]\$$/.test(text)) {
          text = text.replace(/^\$/, '\\(').replace(/\$$/, '\\)');
          code.textContent = text;
        }
        if (/^\\\((.|\s)+\\\)$/.test(text) ||
            /^\\\[(.|\s)+\\\]$/.test(text) ||
            /^\$(.|\s)+\$$/.test(text) ||
            /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
          code.outerHTML = code.innerHTML;  
          continue;
        }
      }
      i++;
    }
</script>

  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css" integrity="sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js" integrity="sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js" integrity="sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



    
  
  
</footer>

      </div>
    </body>
</html>
