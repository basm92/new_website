---
title: The Performance of Probabilistic Latent Semantic Analysis
author: 'Bas Machielsen'
date: '2023-10-05'
excerpt: An introduction to Probabilistic Latent Semantic Analysis followed by an example.
slug: []
categories: []
tags: []
---



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In this blog post, I want to investigate the performance of probabilistic latent semantic analysis: a subject I have been teaching (but also studying) for a course. Probabilistic latent semantic analysis proceeds from a <em>document-term</em> matrix, a standard data matrix in the field of text mining. It should look something like this, where the rows of the matrix represent <span class="math inline">\(n\)</span> documents and the columns <span class="math inline">\(p\)</span> terms (words). Usually, <span class="math inline">\(p &gt; n\)</span>. a</p>
<p><span class="math display">\[
A = \begin{pmatrix}
doc_1,term_1 &amp; doc_1, term_2 &amp; \dots &amp; doc_1, term_p \\\
\vdots &amp; \dots &amp; \ddots &amp; \vdots \\\
doc_n, term_1 &amp; \dots &amp; \dots &amp; doc_n, term_p \end{pmatrix}
\]</span>
The standard maximum likelihood estimator for <span class="math inline">\(Pr(d_i, t_j)\)</span> is <span class="math inline">\(x_{ij} / m\)</span> where <span class="math inline">\(m\)</span> is the total word count in all documents. This has a simple interpretation: count of word <span class="math inline">\(j\)</span> in document <span class="math inline">\(i\)</span> / total word count in all documents.</p>
</div>
<div id="plsa" class="section level2">
<h2>PLSA</h2>
<p>Probabilistic Latent Semantic Analysis (PLSA) is an attempt to decompose this matrix using something similar to a singular value decomposition. In particular, given a probability matrix:</p>
<p><span class="math display">\[
P = \begin{pmatrix} p(d_1, t_1) &amp; \dots &amp; p(d_1, t_p) \\\
\vdots &amp; \ddots &amp; \vdots \\\
p(d_n, t_1)&amp;  \dots &amp; p(d_n, t_p) \end{pmatrix}
\]</span></p>
<p>We can have construct an approximation <span class="math inline">\(U\Sigma V^T\)</span> with <span class="math inline">\(U=N \times r\)</span> (<span class="math inline">\(r\)</span> classes):</p>
<p><span class="math display">\[
U = \begin{pmatrix} p(d_1 | c_1) &amp; \dots &amp; p(d_1 | c_r) \\\
\vdots &amp;  &amp; \vdots \\\
p(d_n | c_1) &amp; \dots &amp; p(d_n | c_r) \end{pmatrix}
\]</span></p>
<p><span class="math inline">\(\Sigma = \text{diag}(P(c_r))\)</span>, and <span class="math inline">\(V^T\)</span> (<span class="math inline">\(r \times p\)</span>) has elements <span class="math inline">\(V^T_{ij} = P(t_j | c_i)\)</span>. Naturally, the object of interest is usually <span class="math inline">\(U\)</span>: this represents the probabilities of the document belonging to class <span class="math inline">\(1\)</span> to <span class="math inline">\(r\)</span>.</p>
<p>In R, the package <code>svs</code> can be used to carry out probabilistic latent semantic analysis:</p>
<pre class="r"><code>library(svs)</code></pre>
<p>In what follows, I’ll demonstrate the capacity of PLSA to distinguish two types of documents on its own: I’ll scrape and convert into a document text matrix several pages about football, and several pages about tennis, set <span class="math inline">\(k\)</span> (the number of classes) equal to 2, and investigate the output.</p>
</div>
<div id="example" class="section level2">
<h2>Example</h2>
<p>Here, I first web-scrape the text of several wikipedia pages:</p>
<p>Now, I use the <code>tidytext</code> package to put these into a document-term matrix:</p>
<pre class="r"><code>library(tidytext)
# Compute the texts into a data.frame
text_df &lt;- tibble(Text = texts) |&gt; 
  rowwise() |&gt; 
  mutate(Text = paste(Text, collapse=&quot;&quot;)) |&gt; 
  ungroup()

# Put all words together grouped by document
text_data &lt;- text_df |&gt;
  group_by(row_number()) |&gt; 
  unnest_tokens(word, Text) |&gt; 
  rename(&#39;document&#39; = &#39;row_number()&#39;)

# Convert to a document-term matrix
# Filter out stop_words and numbers
stop_words &lt;- bind_rows(stop_words, data.frame(word = as.character(0:10000), lexicon=&quot;Custom&quot;))

dtm &lt;- text_data |&gt;
  count(document, word) |&gt;
  filter(!is.element(word, stop_words)) |&gt; #!str_detect(word, paste(as.character(0:10000), collapse=&quot;|&quot;))) |&gt; 
  cast_dtm(document, word, n)</code></pre>
<p>The document-term matrix (<code>dtm</code>) looks like this:</p>
<pre class="r"><code>as.data.frame(as.matrix(dtm)) |&gt; dim()</code></pre>
<pre><code>## [1]   14 6197</code></pre>
<p>Now, let’s compute the frequencies and apply LPSA:</p>
<pre class="r"><code>library(svs)
X &lt;- as.matrix(dtm)
out &lt;- fast_plsa(X, k=2, symmetric=T)</code></pre>
<p>Now, I want to find out which class each of the documents have been assigned to:</p>
<pre class="r"><code>apply(
  out$prob1, 1, which.max
)</code></pre>
<pre><code>##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 
##  1  2  1  1  1  2  2  2  2  2  2  2  2  2</code></pre>
<p>.. which means that the majority of the documents is classified in the correct corresponding cluster.</p>
</div>
<div id="comparison" class="section level2">
<h2>Comparison</h2>
<p>We can compare the results with a so-called latent semantic analysis, which is just a singular value decomposition.</p>
<pre class="r"><code>out_lsa &lt;- fast_lsa(X)

out_lsa$pos1[, &#39;Dim1&#39;]</code></pre>
<pre><code>##            1            2            3            4            5            6 
## -0.803081398 -0.338888792 -0.191561592 -0.298909337 -0.271174528 -0.133052909 
##            7            8            9           10           11           12 
## -0.146478395 -0.026522482 -0.011199147 -0.008402141 -0.019986480 -0.013202093 
##           13           14 
## -0.001734649 -0.001084412</code></pre>
<p>In this case, we can see that the median of the first dimension already separates the documents perfectly in two classes. The first 7 observations having very low values and the second 7 values having very high values. So we can take this to be an indicator for which class the documents belong to:</p>
<pre class="r"><code>data.frame(doc_no = 1:14) |&gt; 
  mutate(class = if_else(out_lsa$pos1[,&#39;Dim1&#39;][doc_no] &gt; median(out_lsa$pos1[,&#39;Dim1&#39;]), 1, 2))</code></pre>
<pre><code>##    doc_no class
## 1       1     2
## 2       2     2
## 3       3     2
## 4       4     2
## 5       5     2
## 6       6     2
## 7       7     2
## 8       8     1
## 9       9     1
## 10     10     1
## 11     11     1
## 12     12     1
## 13     13     1
## 14     14     1</code></pre>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>In this setting, I have demonstrated a simple example of latent probabilistic semantic analysis, and latent semantic analysis, and I would prefer a simpler method to a potentially more complicated method. Thank you for reading!</p>
</div>
